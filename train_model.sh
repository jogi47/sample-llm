#!/bin/bash
set -e

echo "Setting up model training environment..."

# Set environment variables for better performance
if [[ "$OSTYPE" == "darwin"* ]]; then
    echo "MacOS detected, setting MPS optimizations"
    export PYTORCH_ENABLE_MPS_FALLBACK=1
    export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
fi

# Ensure dependencies are installed
echo "Installing training dependencies with uv..."
uv pip install "datasets>=2.15.0" "transformers>=4.38.0" "huggingface_hub>=0.20.3" \
               "torch>=2.2.0" "accelerate>=0.27.0" "safetensors>=0.4.0" "tqdm>=4.66.0" \
               "numpy>=1.26.0" "peft>=0.7.0" --quiet

# Check if HF_TOKEN is set
if [ -z "$HF_TOKEN" ]; then
    echo "⚠️  HF_TOKEN environment variable not set. Model will be trained but not pushed to Hub."
    echo "To set the token, run: export HF_TOKEN=your_huggingface_token"
    PUSH_ARG=""
else
    echo "✓ HF_TOKEN found. Model will be pushed to Hugging Face Hub."
    PUSH_ARG="--push_to_hub"
fi

# Default values
MODEL=${MODEL:-"TinyLlama/TinyLlama-1.1B-Chat-v1.0"}
DATASET=${DATASET:-"knkarthick/dialogsum"}
TEXT_COLUMN=${TEXT_COLUMN:-"dialogue"}
BATCH_SIZE=${BATCH_SIZE:-4}
EPOCHS=${EPOCHS:-1}
MAX_SAMPLES=${MAX_SAMPLES:-2000}
REPO_NAME=${REPO_NAME:-""}

# Get the platform-specific command to check memory
if [[ "$OSTYPE" == "darwin"* ]]; then
    TOTAL_RAM_GB=$(sysctl hw.memsize | awk '{print int($2/1024/1024/1024)}')
else
    TOTAL_RAM_GB=$(free -g | awk 'NR==2 {print $2}')
fi

# Adjust batch size based on available memory
if [ "$TOTAL_RAM_GB" -lt 8 ]; then
    ADJUSTED_BATCH_SIZE=1
    echo "⚠️  Low memory detected (${TOTAL_RAM_GB}GB). Adjusting batch size to ${ADJUSTED_BATCH_SIZE}."
    BATCH_SIZE=$ADJUSTED_BATCH_SIZE
fi

# Print training configuration
echo ""
echo "Training Configuration:"
echo "----------------------"
echo "Model:       $MODEL"
echo "Dataset:     $DATASET"
echo "Text Column: $TEXT_COLUMN"
echo "Batch Size:  $BATCH_SIZE"
echo "Epochs:      $EPOCHS"
echo "Max Samples: $MAX_SAMPLES"
echo "Repository:  ${REPO_NAME:-"<auto-generated>"}"
echo "Push to Hub: ${HF_TOKEN:+"Yes"}"
echo "----------------------"
echo ""

# Confirm with user
read -p "Proceed with training? (y/n) " -n 1 -r
echo ""
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "Training cancelled."
    exit 1
fi

# Run the training script
echo "Starting training..."
uv run train.py \
    --model_name "$MODEL" \
    --dataset_name "$DATASET" \
    --text_column "$TEXT_COLUMN" \
    --batch_size "$BATCH_SIZE" \
    --num_epochs "$EPOCHS" \
    --max_samples "$MAX_SAMPLES" \
    ${REPO_NAME:+"--repository_name $REPO_NAME"} \
    $PUSH_ARG

echo ""
echo "Training complete!"
echo ""
echo "To update config.py to use this model, check the output above for the model ID." 