from fastapi import FastAPI, Body, HTTPException, Query, Request
from fastapi.responses import HTMLResponse, RedirectResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel, Field
import uvicorn
from typing import List, Dict, Any, Optional, Union
import os
from pathlib import Path
from model import llm_handler
from config import Config
import chat

# Create FastAPI app with enhanced documentation
app: FastAPI = FastAPI(
    title="Pyth API",
    description="""
    A FastAPI application with item management and local LLM integration.
    
    ## Features
    
    * Store and retrieve items
    * Generate text with local LLM models
    * View model information
    * Chat interface
    
    Models are optimized to run on macOS with 8GB RAM.
    """,
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json"
)

# Setup static file serving
static_path = Path(__file__).parent / "static"
if static_path.exists():
    app.mount("/static", StaticFiles(directory=str(static_path)), name="static")

# Setup templates
templates_path = Path(__file__).parent / "templates"
if not templates_path.exists():
    templates_path.mkdir(parents=True, exist_ok=True)

templates = Jinja2Templates(directory=str(templates_path))

# In-memory storage for items
items: List[Dict[str, Any]] = []

class Item(BaseModel):
    name: str = Field(..., description="The name of the item", example="Sample Item")
    description: Optional[str] = Field(None, description="Optional description of the item", example="This is a sample item")
    value: Optional[int] = Field(None, description="Optional numerical value", example=100)

class PromptRequest(BaseModel):
    prompt: str = Field(..., description="The text prompt to send to the LLM", example="What is the capital of France?")
    max_length: Optional[int] = Field(512, description="Maximum length of the generated response", example=512)
    temperature: Optional[float] = Field(0.7, description="Temperature for text generation (higher = more creative)", example=0.7)
    top_p: Optional[float] = Field(0.95, description="Top-p sampling parameter", example=0.95)

    class Config:
        schema_extra = {
            "example": {
                "prompt": "What is the capital of France?",
                "max_length": 512,
                "temperature": 0.7,
                "top_p": 0.95
            }
        }

class LLMResponse(BaseModel):
    response: str = Field(..., description="The text generated by the LLM model")
    model: str = Field(..., description="The name of the model used for generation")

class ModelInfo(BaseModel):
    current_model: str = Field(..., description="The currently loaded model name")
    available_models: Dict[str, Dict[str, Any]] = Field(..., description="Information about all available models")

# Include the chat router
app.include_router(chat.router)

@app.get("/api", 
    response_class=RedirectResponse,
    summary="API Root endpoint",
    description="Redirects to the API documentation page",
    status_code=302,
    tags=["API"])
def read_api_root() -> RedirectResponse:
    return RedirectResponse(url="/docs")

@app.get("/welcome", 
    summary="Welcome message",
    description="Returns a simple welcome message to confirm the API is running",
    response_description="Welcome message object",
    tags=["General"])
def welcome() -> Dict[str, str]:
    return {"message": "Welcome to Pyth API"}

@app.get("/items", 
    summary="Get all items",
    description="Retrieves all items currently stored in the collection",
    response_description="List of all items",
    tags=["Items"])
def get_items() -> Dict[str, List[Dict[str, Any]]]:
    return {"items": items}

@app.post("/items", 
    status_code=201,
    summary="Add a new item",
    description="Adds a new item to the collection with the provided details",
    response_description="The added item and success status",
    tags=["Items"])
def add_item(item: Item) -> Dict[str, Union[str, Item]]:
    items.append(item.model_dump())
    return {"status": "success", "item": item}

@app.post("/llm/generate", 
    response_model=LLMResponse,
    summary="Generate text with LLM",
    description="""
    Generates text using the local LLM model based on the provided prompt.
    
    The model will process your prompt and return an AI-generated completion.
    You can control generation parameters like temperature and max length.
    """,
    response_description="The generated text response",
    tags=["LLM"])
def generate_llm_response(request: PromptRequest) -> LLMResponse:
    """Generate a response from the LLM model"""
    try:
        response: str = llm_handler.generate_response(
            prompt=request.prompt,
            max_length=request.max_length or 512,
            temperature=request.temperature or 0.7,
            top_p=request.top_p or 0.95
        )
        return LLMResponse(response=response, model=llm_handler.model_name)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LLM error: {str(e)}")

@app.get("/llm/info", 
    response_model=ModelInfo,
    summary="Get LLM model information",
    description="Retrieves information about the current LLM model and all available models",
    response_description="Current model and available model information",
    tags=["LLM"])
def get_model_info() -> ModelInfo:
    """Get information about the current LLM model and available models"""
    return ModelInfo(
        current_model=llm_handler.model_name,
        available_models=Config.AVAILABLE_MODELS
    )

@app.on_event("startup")
async def startup_event() -> None:
    """Load the LLM model when the application starts"""
    # Preload model in a non-blocking way
    print("Preloading LLM model...")
    llm_handler.load_model()

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
